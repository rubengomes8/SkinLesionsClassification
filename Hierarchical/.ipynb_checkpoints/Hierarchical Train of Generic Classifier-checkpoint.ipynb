{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the classifier to be trained and the classes array\n",
    "# a: ['MEL', 'NMEL']\n",
    "# b: ['MEL', 'NV']\n",
    "# c: ['BEN', 'MAL']\n",
    "# d: ['BKL', 'DF', 'VASC']\n",
    "# e: ['BCC', 'AKIEC']\n",
    "\n",
    "classifier = 'a'\n",
    "classesArray = ['BCC', 'AKIEC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from numpy import array\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports all the images from a specified folder, with a specific extension\n",
    "# and resizes to a specific imgHeight, imgWidth\n",
    "\n",
    "def import_dataset(path, mode, fileExtension='.jpg', imgWidth=224, imgHeight=224):\n",
    "    datasetFilenamesImages = []\n",
    "    dataset = []\n",
    "    print(\"Start importing \" + mode + \" images...\")\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(fileExtension): \n",
    "            completePath = os.path.join(path, filename)\n",
    "            image = cv2.imread(completePath, cv2.IMREAD_COLOR)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, dsize=(imgHeight, imgWidth), interpolation=cv2.INTER_AREA)\n",
    "            filenameImage = [filename, image]\n",
    "            datasetFilenamesImages.append(filenameImage)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    datasetFilenamesImages = sorted(datasetFilenamesImages, key=itemgetter(0))\n",
    "    for x in datasetFilenamesImages:\n",
    "        dataset.append(x[1])\n",
    "    \n",
    "    return array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(path_groundtruth, classesArray):\n",
    "    target = []\n",
    "    if len(classesArray) == 2:\n",
    "        counter = {classesArray[0]: 0, classesArray[1]: 0}\n",
    "    elif len(classesArray) == 3:\n",
    "        counter = {classesArray[0]: 0, classesArray[1]: 0, classesArray[2]: 0}\n",
    "        \n",
    "    i = 0\n",
    "    with open(path_groundtruth, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if row[1] == '1.0': # BCC\n",
    "                counter[classesArray[0]] += 1\n",
    "                target.append(0)\n",
    "            elif row[2] == '1.0': # AKIEC\n",
    "                counter[classesArray[1]] += 1\n",
    "                target.append(1)\n",
    "                \n",
    "            if len(classesArray) == 3:\n",
    "                if row[3] == '1.0':\n",
    "                    counter[classesArray[2]] += 1\n",
    "                    target.append(2)\n",
    "\n",
    "    print(counter)\n",
    "    file.close()\n",
    "    return counter, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_train_error(fit):\n",
    "    plt.plot(fit.history['accuracy'])\n",
    "    plt.plot(fit.history['val_accuracy'])\n",
    "    plt.grid(True)\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(fit.history['loss'])\n",
    "    plt.plot(fit.history['val_loss'])\n",
    "    plt.grid(True)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model = 'densenet', transferLearning=True, noClasses=2):\n",
    "    if model == 'resnet':\n",
    "\n",
    "        from tensorflow.keras.applications.resnet import ResNet101\n",
    "        from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "        if transferLearning == False:\n",
    "            resnet = ResNet101(include_top=False, weights=None, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "        else:\n",
    "            resnet = ResNet101(include_top=False, weights='imagenet', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "        model = tf.keras.Sequential(resnet)\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "        model.add(Dense(units=noClasses, activation=\"softmax\"))\n",
    "\n",
    "    elif model == 'densenet':\n",
    "\n",
    "        from tensorflow.keras.applications.densenet import DenseNet121\n",
    "        from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "        if transferLearning == False:\n",
    "            densenet = DenseNet121(include_top=False, weights=None, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "        else:\n",
    "            densenet = DenseNet121(include_top=False, weights='imagenet', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "        model = tf.keras.Sequential(densenet)\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "        model.add(Dense(units=noClasses, activation=\"softmax\"))\n",
    "\n",
    "    else:\n",
    "        print(\"That model is not available.\")\n",
    "        exit(0)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler\n",
    "def scheduler(epoch, noEpochs, learningRate):\n",
    "    if epoch < int(0.5 * noEpochs):\n",
    "        return learningRate\n",
    "    elif epoch < int(0.75 * noEpochs):\n",
    "        return learningRate / 10\n",
    "    else:\n",
    "        return learningRate / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Training and Validation datasets and Groundtruth of classifier\n",
    "classifierToIndex = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4}\n",
    "indexClassifier = classifierToIndex[classifier]\n",
    "\n",
    "pathTrainArray = ['/c/Users/ruben/Desktop/train2018',\n",
    "                  '/c/Users/ruben/Desktop/hier/b',\n",
    "                  '/c/Users/ruben/Desktop/hier/c',\n",
    "                  '/c/Users/ruben/Desktop/hier/d',\n",
    "                  '/c/Users/ruben/Desktop/hier/e']\n",
    "\n",
    "truthTrainArray = ['/c/Users/ruben/Desktop/hier/a/labels.csv',\n",
    "                   '/c/Users/ruben/Desktop/hier/b/labels.csv',\n",
    "                   '/c/Users/ruben/Desktop/hier/c/labels.csv',\n",
    "                   '/c/Users/ruben/Desktop/hier/d/labels.csv',\n",
    "                   '/c/Users/ruben/Desktop/hier/e/labels.csv',]\n",
    "\n",
    "pathValArray  = ['/c/Users/ruben/Desktop/val2018',\n",
    "                 '/c/Users/ruben/Desktop/hier_val/b',\n",
    "                 '/c/Users/ruben/Desktop/hier_val/c',\n",
    "                 '/c/Users/ruben/Desktop/hier_val/d',\n",
    "                 '/c/Users/ruben/Desktop/hier_val/e']\n",
    "\n",
    "truthValArray = ['/c/Users/ruben/Desktop/hier_val/a/labels.csv',\n",
    "                 '/c/Users/ruben/Desktop/hier_val/b/labels.csv',\n",
    "                 '/c/Users/ruben/Desktop/hier_val/c/labels.csv',\n",
    "                 '/c/Users/ruben/Desktop/hier_val/d/labels.csv',\n",
    "                 '/c/Users/ruben/Desktop/hier_val/e/labels.csv']\n",
    "\n",
    "x_train = import_dataset(pathTrainArray[indexClassifier], 'training')\n",
    "counter, y_train = assign_labels(truthTrainArray[indexClassifier], classesArray)\n",
    "\n",
    "x_val = import_dataset(pathValArray[indexClassifier], 'validation')\n",
    "counter, y_val = assign_labels(truthValArray[indexClassifier], classesArray)\n",
    "\n",
    "print(\"Dataset Imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training variables\n",
    "noEpochs = 20\n",
    "learningRate = 1e-5\n",
    "noClasses = len(classesArray)\n",
    "batchSize = 10\n",
    "modelName = 'densenet'\n",
    "\n",
    "\n",
    "\n",
    "checkpointPath = {\n",
    "    'resnet': [\"/c/Users/ruben/Desktop/ModelWeights/Hier/a/resnet/cp.ckpt\",\n",
    "               \"/c/Users/ruben/Desktop/ModelWeights/Hier/b/resnet/cp.ckpt\",\n",
    "               \"/c/Users/ruben/Desktop/ModelWeights/Hier/c/resnet/cp.ckpt\",\n",
    "               \"/c/Users/ruben/Desktop/ModelWeights/Hier/d/resnet/cp.ckpt\",\n",
    "               \"/c/Users/ruben/Desktop/ModelWeights/Hier/e/resnet/cp.ckpt\"],\n",
    "    'densenet': [\"/c/Users/ruben/Desktop/ModelWeights/Hier/a/densenet/cp.ckpt\",\n",
    "                \"/c/Users/ruben/Desktop/ModelWeights/Hier/b/densenet/cp.ckpt\",\n",
    "                \"/c/Users/ruben/Desktop/ModelWeights/Hier/c/densenet/cp.ckpt\",\n",
    "                \"/c/Users/ruben/Desktop/ModelWeights/Hier/d/densenet/cp.ckpt\",\n",
    "                \"/c/Users/ruben/Desktop/ModelWeights/Hier/e/densenet/cp.ckpt\"]\n",
    "}\n",
    "\n",
    "w_0_float = float(counter[classesArray[0]])\n",
    "w_1_float = float(counter[classesArray[1]])\n",
    "\n",
    "\n",
    "if noClasses == 2:\n",
    "    w_total = w_0_float + w_1_float\n",
    "    w_0 = w_total / w_0_float\n",
    "    w_1 = w_total / w_1_float\n",
    "    classWeights = {0: w_0, 1: w_1}\n",
    "elif noClasses == 3:\n",
    "    w_2_float = float(counter[classesArray[2]])\n",
    "    w_total = w_0_float + w_1_float + w_2_float\n",
    "    w_0 = w_total / w_0_float\n",
    "    w_1 = w_total / w_1_float\n",
    "    w_2 = w_total / w_2_float\n",
    "    classWeights = {0: w_0, 1: w_1, 2: w_2}\n",
    "\n",
    "y_train_cat = keras.utils.to_categorical(y_train, noClasses)\n",
    "y_val_cat = keras.utils.to_categorical(y_val, noClasses)\n",
    "\n",
    "# Callback to adapt the learning rate value\n",
    "schedulerLearningRate = tf.keras.callback.LearningRateScheduler(scheduler)\n",
    "\n",
    "# Callback to save the Best Model Weights\n",
    "checkpoint_dir = os.path.dirname(checkpointPath[modelName][index])\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpointPath[modelName][index],\n",
    "    save_weights_only=True,\n",
    "    verbose=1, \n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the model\n",
    "model = create_model(modelName, transferLearning=True, noClasses)\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss = keras.losses.categorical_crossentropy,\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=1e-5),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train_cat,\n",
    "    batch_size=batchSize,\n",
    "    class_weight=classWeights,\n",
    "    callbacks=[schedulerLearningRate, checkpoint],\n",
    "    epochs = noEpochs,\n",
    "    shuffle=True\n",
    "    validation_data=(x_val, y_val_cat)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate in validation set\n",
    "model.load_weights(checkpointPath[modelName])\n",
    "\n",
    "y_pred = model.predict_classes(x_val)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "score = model.evaluate(x_val, y_val_cat, verbose=1)\n",
    "print(f'Val loss: {score[0]} / Val accuracy: {score[1]}')\n",
    "\n",
    "plot_val_train_error(fit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
